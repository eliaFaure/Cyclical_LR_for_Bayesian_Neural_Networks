{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b556915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdef36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define mean and std for normalization\n",
    "\n",
    "mean = [0.5071, 0.4867, 0.4408]\n",
    "std = [0.2675, 0.2565, 0.2761]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad28a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Define the transformations for training and validation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  \n",
    "    transforms.RandomHorizontalFlip(),      \n",
    "    transforms.Normalize(mean, std),       \n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),      \n",
    "])\n",
    "\n",
    "# Load the full CIFAR-100 dataset \n",
    "full_trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True)\n",
    "\n",
    "# Split the full dataset into training and validation sets \n",
    "train_size = int(0.9 * len(full_trainset))\n",
    "val_size = len(full_trainset) - train_size\n",
    "trainset, valset = random_split(full_trainset, [train_size, val_size])\n",
    "\n",
    "# Apply the appropriate transformations to each subset\n",
    "\n",
    "trainset = torch.utils.data.dataset.Subset(full_trainset, range(train_size))\n",
    "valset = torch.utils.data.dataset.Subset(full_trainset, range(train_size, len(full_trainset)))\n",
    "\n",
    "trainset.dataset.transform = transform_train  \n",
    "valset.dataset.transform = transform_val      \n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=0)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=100, shuffle=False, num_workers=0)\n",
    "\n",
    "# Test set \n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_val)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "epochs = 200\n",
    "epochs_to_run=200\n",
    "lr_0 = 0.1  # initial learning rate\n",
    "weight_decay = 5e-4\n",
    "alpha = 0.9\n",
    "temperature = 1.0 / 50000 #used only when noise injection\n",
    "datasize = 50000\n",
    "M = 4  # define number of cycles\n",
    "num_batch = len(trainloader)  \n",
    "T = epochs * num_batch\n",
    "criterion = nn.CrossEntropyLoss() #used for computing loss\n",
    "\n",
    "\n",
    "mt = 0  # checkpoint number\n",
    "save_dir = \"./checkpoints\"  \n",
    "os.makedirs(save_dir, exist_ok=True)  \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print (device)\n",
    "\n",
    "# Step 1: Rebuild the model architecture exactly as before\n",
    "net = resnet18(num_classes=100) \n",
    "net = net.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38caa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINE CYCLICAL FUNCTIONS\n",
    "\n",
    "def update_params_csgld(lr, epoch, weight_decay):\n",
    "    \"\"\"update parameters using cyclical SGLD, add noise to the gradient only in the last 5 epochs of each cycle\"\"\"\n",
    "    for p in net.parameters():\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "        d_p = p.grad.data\n",
    "        d_p.add_(weight_decay, p.data)\n",
    "\n",
    "        if (epoch % 50) + 1 > 45:\n",
    "            eps = torch.randn(p.size()).cuda(device)\n",
    "            noise_term = (2.0 * lr * temperature / datasize) ** 0.5 * eps\n",
    "            p.data.add_(-lr, d_p)\n",
    "            p.data.add_(noise_term)\n",
    "        else:\n",
    "            p.data.add_(-lr, d_p)\n",
    "\n",
    "\n",
    "\n",
    "def update_params_chmcmc(alpha,temperature,datasize,lr,epoch,weight_decay):\n",
    "    \"\"\"update parameters using  cyclical Hamiltonian montecarlo (use momentum), add noise to the gradient only in the last 5 epochs of each cycle\"\"\"\n",
    "    for p in net.parameters():\n",
    "        if not hasattr(p,'buf'):\n",
    "            p.buf = torch.zeros(p.size()).cuda(device)\n",
    "        d_p = p.grad.data\n",
    "        d_p.add_(weight_decay, p.data)\n",
    "        buf_new = (1-alpha)*p.buf - lr*d_p\n",
    "\n",
    "        if (epoch%50)+1>45:\n",
    "            eps = torch.randn(p.size()).cuda(device)\n",
    "            buf_new += (2.0*lr*alpha*temperature/datasize)**.5*eps\n",
    "        p.data.add_(buf_new)\n",
    "        p.buf = buf_new\n",
    "\n",
    "\n",
    "def adjust_learning_rate_cyclical(epoch, batch_idx, num_batch, T, lr_0=0.1, M=4):\n",
    "    \"\"\"implementation of cyclical learning rate schedule\"\"\"\n",
    "    \n",
    "    rcounter = epoch*num_batch+batch_idx\n",
    "    cos_inner = np.pi * (rcounter % (T // M))\n",
    "    cos_inner /= T // M\n",
    "    cos_out = np.cos(cos_inner) + 1\n",
    "    lr = 0.5*cos_out*lr_0\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b042b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,trainLoader,alpha,temperature, datasize, num_batch, epoch,tot_epochs, weight_decay,T):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    global_iter = (epoch) * num_batch \n",
    "    print(\"global iter\",global_iter)\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainLoader):\n",
    "        if device == 'cuda':\n",
    "            inputs, targets = inputs.cuda(device), targets.cuda(device)\n",
    "            \n",
    "        net.zero_grad()\n",
    "        lr = adjust_learning_rate_cyclical(epoch, batch_idx, num_batch, T)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        update_params_chmcmc(alpha,temperature,datasize,lr,epoch,weight_decay)\n",
    "\n",
    "        train_loss += loss.data.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        if batch_idx%100==0:\n",
    "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (train_loss/(batch_idx+1), 100.*correct.item()/total, correct, total),f\"learnig rate {lr,global_iter}\")\n",
    "            \n",
    "        global_iter+=1\n",
    "        \n",
    "\n",
    "    train_accuracy = 100. * correct.item() / total\n",
    "    return train_loss / len(trainLoader),  train_accuracy,lr\n",
    "    \n",
    "\n",
    "def test(net,valLoader,epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valLoader):\n",
    "            if device == 'cuda':\n",
    "                inputs, targets = inputs.cuda(device), targets.cuda(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.data.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "            if batch_idx%100==0:\n",
    "                print('Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)'\n",
    "                    % (test_loss/(batch_idx+1), 100.*correct.item()/total, correct, total))\n",
    "\n",
    "    test_accuracy = 100. * correct.item() / total\n",
    "\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "    test_loss/len(valLoader), correct, total,\n",
    "    100. * correct.item() / total))\n",
    "\n",
    "    return test_loss/len(valLoader), test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4111d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize wandb, fill with your data\n",
    "import wandb \n",
    "wandb.login(key=\"\")\n",
    "\n",
    "run_name = f\"\"\n",
    "\n",
    "wandb.init(project='', name=run_name, config={\n",
    "        'learning_rate': alpha,\n",
    "        'batch_size': datasize,\n",
    "        'epochs': epochs,\n",
    "        'weight_decay': weight_decay,\n",
    "        'temperature': temperature\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa18e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE MODEL\n",
    "\n",
    "saved_epochs=set()\n",
    "total_models_to_save = 12\n",
    "last_epochs_to_save = 3\n",
    "random_models_to_save = total_models_to_save - last_epochs_to_save\n",
    "mt = 0  #used for naming random saved models\n",
    "\n",
    "print(num_batch)\n",
    "\n",
    "random_saved_epochs = sorted(random.sample(range(epochs - epochs_to_run, epochs - last_epochs_to_save), random_models_to_save))\n",
    "final_saved_epochs = list(range(epochs - last_epochs_to_save, 200))  \n",
    "all_saved_epochs = set(random_saved_epochs + final_saved_epochs)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs-epochs_to_run,epochs_to_run):\n",
    "    # Training loop\n",
    "    train_loss, train_accuracy,learn_rate=train(net, trainloader,alpha,temperature,datasize,num_batch, epoch,epochs,weight_decay,T)\n",
    "    \n",
    "    # Test loop\n",
    "    test_loss, test_accuracy=test(net, valloader, epoch)\n",
    "\n",
    "    wandb.log({\n",
    "        'train_loss': train_loss / len(trainloader),\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_loss': test_loss,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"epoch\": epoch,\n",
    "        \"lr\":learn_rate\n",
    "    })\n",
    "\n",
    "    \n",
    "    if (epoch % 50) + 1 > 47:  # Save 3 models per cycle, during the last 3 epochs of each cycle, where noose is injected\n",
    "        print(f\"Saving model snapshot at epoch {epoch}\")\n",
    "        net.cpu()\n",
    "        torch.save(net.state_dict(), f'{save_dir}/cifar100_csghmc0.5_{mt}.pt')\n",
    "        mt += 1\n",
    "        net.cuda(device)\n",
    "\n",
    "         \n",
    "        \n",
    "print(saved_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kvpress",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
