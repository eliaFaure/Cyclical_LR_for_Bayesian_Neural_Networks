{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b242db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3d65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define mean and std for normalization\n",
    "\n",
    "mean = [0.5071, 0.4867, 0.4408]\n",
    "std = [0.2675, 0.2565, 0.2761]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f6da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Define the transformations for training and validation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  \n",
    "    transforms.RandomHorizontalFlip(),      \n",
    "    transforms.Normalize(mean, std),       \n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),      \n",
    "])\n",
    "\n",
    "# Load the full CIFAR-100 dataset \n",
    "full_trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True)\n",
    "\n",
    "# Split the full dataset into training and validation sets \n",
    "train_size = int(0.9 * len(full_trainset))\n",
    "val_size = len(full_trainset) - train_size\n",
    "trainset, valset = random_split(full_trainset, [train_size, val_size])\n",
    "\n",
    "# Apply the appropriate transformations to each subset\n",
    "\n",
    "trainset = torch.utils.data.dataset.Subset(full_trainset, range(train_size))\n",
    "valset = torch.utils.data.dataset.Subset(full_trainset, range(train_size, len(full_trainset)))\n",
    "\n",
    "trainset.dataset.transform = transform_train  \n",
    "valset.dataset.transform = transform_val      \n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=0)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=100, shuffle=False, num_workers=0)\n",
    "\n",
    "# Test set \n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_val)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "epochs = 200\n",
    "epochs_to_run=200\n",
    "lr_0 = 0.1  # initial learning rate\n",
    "weight_decay = 5e-4\n",
    "alpha = 0.9\n",
    "temperature = 1.0 / 50000\n",
    "datasize = 50000\n",
    "M = 4  # number of cycles, change as needed\n",
    "num_batch = len(trainloader)  \n",
    "T = epochs * num_batch\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "mt = 0  # checkpoint number\n",
    "save_dir = \"./checkpoints\"  \n",
    "os.makedirs(save_dir, exist_ok=True)  \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print (device)\n",
    "\n",
    "# Step 1: Rebuild the model architecture exactly as before\n",
    "net = resnet18(num_classes=100) \n",
    "net = net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33467ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params_sghmc(alpha,temperature,datazise, lr, epoch,tot_epochs):\n",
    "    \"\"\"standard Hamiltoniano Monte Carlo use momentum and noise in the last 50 epochs, first 150 are for burning\"\"\"\n",
    "    for p in net.parameters():\n",
    "        if not hasattr(p, 'buf'):\n",
    "            p.buf = torch.zeros(p.size()).cuda(device)\n",
    "            \n",
    "        d_p = p.grad.data\n",
    "        d_p.add_(weight_decay, p.data)\n",
    "        buf_new = (1 - alpha) * p.buf - lr * d_p \n",
    "\n",
    "        #add noise only on the last 50 epochs \n",
    "        if epoch >= tot_epochs - 50: \n",
    "            noise = torch.randn_like(p.data).cuda(device)\n",
    "            buf_new += (2.0 * lr * temperature / datasize) ** 0.5 * noise\n",
    "\n",
    "        p.data.add_(buf_new)\n",
    "        p.buf = buf_new\n",
    "\n",
    "def update_params_sgld( lr, epoch,tot_epochs, weight_decay):\n",
    "    \"\"\"standard Langevin dynamics with noise in the last 50 epochs, first 150 are for burning\"\"\"\n",
    "        \n",
    "    for p in net.parameters():\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "\n",
    "        # Add L2 regularization (weight decay) to the gradient\n",
    "        grad = p.grad.data + weight_decay * p.data\n",
    "\n",
    "       # Inject noise only in the final 50 epochs\n",
    "        if epoch >= tot_epochs - 50:\n",
    "            noise = torch.randn_like(p.data).cuda(device)\n",
    "            noise_term = (2 * lr* temperature / datasize) ** 0.5 * noise\n",
    "            p.data -= lr * grad - noise_term\n",
    "        else:\n",
    "            # Update parameters without noise in the burn-in phase \n",
    "            p.data -= lr * grad\n",
    "\n",
    "\n",
    "def adjust_learning_rate_standard(t, a=0.1, b=1.0, gamma=0.55, limit_iter=500):\n",
    "    \"\"\"standard learning rate decay\"\"\"\n",
    "    learnig_rate=a\n",
    "    if t >= limit_iter:\n",
    "        learnig_rate= a * (b + t-limit_iter) ** (-gamma)\n",
    "    return learnig_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520cac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,trainLoader,alpha,temperature, datasize, num_batch, epoch,tot_epochs, weight_decay):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    global_iter = (epoch) * num_batch \n",
    "    print(\"global iter\",global_iter)\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainLoader):\n",
    "        if device == 'cuda':\n",
    "            inputs, targets = inputs.cuda(device), targets.cuda(device)\n",
    "            \n",
    "        net.zero_grad()\n",
    "        lr = adjust_learning_rate_standard(global_iter)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        update_params_sgld(lr, epoch , tot_epochs, weight_decay)\n",
    "\n",
    "        train_loss += loss.data.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        if batch_idx%100==0:\n",
    "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (train_loss/(batch_idx+1), 100.*correct.item()/total, correct, total),f\"learnig rate {lr,global_iter}\")\n",
    "            \n",
    "        global_iter+=1\n",
    "        \n",
    "\n",
    "    train_accuracy = 100. * correct.item() / total\n",
    "    return train_loss / len(trainLoader),  train_accuracy,lr\n",
    "    \n",
    "\n",
    "def test(net,valLoader,epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valLoader):\n",
    "            if device == 'cuda':\n",
    "                inputs, targets = inputs.cuda(device), targets.cuda(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.data.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "            if batch_idx%100==0:\n",
    "                print('Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)'\n",
    "                    % (test_loss/(batch_idx+1), 100.*correct.item()/total, correct, total))\n",
    "\n",
    "    test_accuracy = 100. * correct.item() / total\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "    test_loss/len(valLoader), correct, total,\n",
    "    100. * correct.item() / total))\n",
    "\n",
    "    return test_loss/len(valLoader), test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize wandb, fill with your data\n",
    "import wandb \n",
    "wandb.login(key=\"\")\n",
    "\n",
    "run_name = f\"\"\n",
    "\n",
    "wandb.init(project='', name=run_name, config={\n",
    "        'learning_rate': alpha,\n",
    "        'batch_size': datasize,\n",
    "        'epochs': epochs,\n",
    "        'weight_decay': weight_decay,\n",
    "        'temperature': temperature\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_epochs=set()\n",
    "total_models_to_save = 12\n",
    "last_epochs_to_save = 3\n",
    "random_models_to_save = total_models_to_save - last_epochs_to_save\n",
    "mt = 0  # This is the model counter used for naming random saved models\n",
    "\n",
    "print(num_batch)\n",
    "\n",
    "random_saved_epochs = sorted(random.sample(range(150, epochs_to_run), random_models_to_save))\n",
    "final_saved_epochs = list(range(epochs - last_epochs_to_save, 200))  # [197, 198, 199]\n",
    "all_saved_epochs = set(random_saved_epochs + final_saved_epochs)\n",
    "\n",
    "print(all_saved_epochs)\n",
    "\n",
    "\n",
    "for epoch in range(epochs-epochs_to_run,epochs_to_run):\n",
    "    # Training loop\n",
    "    train_loss, train_accuracy,learn_rate=train(net, trainloader,alpha,temperature,datasize,num_batch, epoch,epochs,weight_decay)\n",
    "    \n",
    "    # Test loop\n",
    "    test_loss, test_accuracy=test(net, valloader, epoch)\n",
    "\n",
    "    wandb.log({\n",
    "        'train_loss': train_loss / len(trainloader),\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_loss': test_loss,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"epoch\": epoch,\n",
    "        \"lr\":learn_rate\n",
    "    })\n",
    "\n",
    "    \n",
    "    # Save 9 random models in the last 50 epochs + last 3 epochs\n",
    "    if epoch in all_saved_epochs:\n",
    "        print(f\"Saving model snapshot at epoch {epoch}\")\n",
    "        model_name = f'{save_dir}/cifar100_sgld_scaled_lr01_{mt}.pt'\n",
    "        print(model_name)\n",
    "        mt+=1\n",
    "        torch.save(net.state_dict(), model_name)\n",
    "        saved_epochs.add(epoch) \n",
    "        net.cuda(device)\n",
    "\n",
    "    #Save burned-in model \n",
    "    if epoch == 149: \n",
    "        print(f\"Saving model snapshot at epoch {epoch}\")\n",
    "        model_name = f'{save_dir}/cifar100_sgld_finalNoNoise.pt'\n",
    "        print(model_name)\n",
    "        mt+=1\n",
    "        torch.save(net.state_dict(), model_name)\n",
    "        saved_epochs.add(epoch)  \n",
    "        net.cuda(device)\n",
    "         \n",
    "        \n",
    "print(saved_epochs)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
